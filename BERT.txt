양방향 인코더
nlp
gpt-1
=> 문장 하나씩 읽어가며 학습하고 결과를 예측
단방향으로 읽어내니 한계가 존재(왼쪽에서 오른쪽)
-Transformer

LM = 쭉 읽어서 학습 후 예측하는 방식
BERT = 문맥을(양방향) 읽어서 가려진 것을 찾아내는 방식
- 질의응답 같은 데이터를 주로 활용
word piece embedding
단어 토큰
